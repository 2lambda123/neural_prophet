{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conformal Prediction using Energy Hospital Load**\n",
    "\n",
    "One example of the NP data is the electricity consumption of a hospital in SF. It has hourly reservation for the entire year of 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown option --versions\n",
      "usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ...\n",
      "Try `python -h' for more information.\n"
     ]
    }
   ],
   "source": [
    "!python --versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Setup\n",
    "Requires Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ourownstory/neural_prophet.git\n",
    "# !cd neural_prophet\n",
    "# !pip install -e \".[dev]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data and Split Data into Train and Test\n",
    "\n",
    "\n",
    "**Extract Data From GitHub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from neuralprophet import NeuralProphet, set_log_level, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"https://raw.githubusercontent.com/ourownstory/neuralprophet-data/main/datasets/\"\n",
    "file = 'energy/SF_hospital_load.csv'\n",
    "# file = 'air_passengers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ywzhdfn2uqLf",
    "outputId": "95decf15-d410-45c9-b703-91fd68891e7f"
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_location + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "TvrgKVWIuxFJ",
    "outputId": "99908203-2022-456a-9d05-73c3d0e6731e"
   },
   "outputs": [],
   "source": [
    "# data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into train, val, cal, and test in that order**\n",
    "\n",
    "Do we need to instantiate a NP model `m` in order to split the df into train and test? If so, does the NP params make any diff to the outcome of this split? I'm presuming no atm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.989% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8213, 2), (547, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = NeuralProphet().split_df(data_df, freq='H', valid_p = 1.0/16)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set time range:    2015-01-01 01:00:00 - 2015-12-09 05:00:00\n",
      "Test set time range:     2015-12-09 06:00:00 - 2016-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set time range:    {train_df['ds'].min()} - {train_df['ds'].max()}\")\n",
    "print(f\"Test set time range:     {test_df['ds'].min()} - {test_df['ds'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folds using CV Splits and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"H\"\n",
    "random_seed = 0\n",
    "B_values = [10, 20, 30, 40]  # Number of bootstraps and number of k-folds\n",
    "val_fold_pct = 0.04          # Validation fold size % of entire input\n",
    "val_cov_pct = 0.8            # Overall validation set coverage %\n",
    "val_fold_size = None         # Fixed validation fold size % of entire fold (hence train fold size is 1 - val_fold_size)\n",
    "                             # Default is None for Extending Window/Rolling Origin, (0, 1) for Fixed Sliding Window/Blocked CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_lo = 0.05\n",
    "quantile_hi = 0.95\n",
    "quantile_lo_str = str(quantile_lo*100)\n",
    "quantile_hi_str = str(quantile_hi*100)\n",
    "quantiles = [quantile_lo, quantile_hi]\n",
    "n_lags = 3*24\n",
    "params = {  # m1 by default, uncomment for m3\n",
    "    'growth': 'off',\n",
    "    'yearly_seasonality': False,\n",
    "    'weekly_seasonality': False,\n",
    "    'daily_seasonality': False,\n",
    "    'n_lags': n_lags,\n",
    "    'ar_reg': 1,\n",
    "    'learning_rate': 0.01,\n",
    "    'quantiles': quantiles,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ds', 'y', 'yhat1', 'residual1', f'yhat1 {quantile_lo_str}%', f'yhat1 {quantile_hi_str}%']\n",
    "cal_forecast_dict = defaultdict(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.988% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 133\n",
      "Epoch[133/133]: 100%|██████████| 133/133 [00:31<00:00,  4.22it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.976% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 129\n",
      "Epoch[129/129]: 100%|██████████| 129/129 [00:32<00:00,  3.97it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.979% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 126\n",
      "Epoch[109/109]: 100%|██████████| 109/109 [01:04<00:00,  1.69it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.988% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.94% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 169\n",
      "Epoch[169/169]: 100%|██████████| 169/169 [00:23<00:00,  7.12it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.95% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 160\n",
      "Epoch[160/160]: 100%|██████████| 160/160 [00:27<00:00,  5.82it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.957% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 153\n",
      "Epoch[153/153]: 100%|██████████| 153/153 [00:29<00:00,  5.17it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.962% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 147\n",
      "Epoch[147/147]: 100%|██████████| 147/147 [00:31<00:00,  4.66it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.966% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 142\n",
      "Epoch[142/142]: 100%|██████████| 142/142 [00:36<00:00,  3.91it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.97% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 138\n",
      "Epoch[138/138]: 100%|██████████| 138/138 [00:40<00:00,  3.42it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.972% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 135\n",
      "Epoch[135/135]: 100%|██████████| 135/135 [00:42<00:00,  3.17it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.975% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 131\n",
      "Epoch[131/131]: 100%|██████████| 131/131 [00:44<00:00,  2.96it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.977% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 129\n",
      "Epoch[129/129]: 100%|██████████| 129/129 [00:46<00:00,  2.78it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.978% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 126\n",
      "Epoch[126/126]: 100%|██████████| 126/126 [00:51<00:00,  2.44it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.98% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 124\n",
      "Epoch[124/124]: 100%|██████████| 124/124 [00:49<00:00,  2.52it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.981% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 122\n",
      "Epoch[122/122]: 100%|██████████| 122/122 [00:47<00:00,  2.57it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.982% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 120\n",
      "Epoch[120/120]: 100%|██████████| 120/120 [00:51<00:00,  2.34it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.983% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 118\n",
      "Epoch[118/118]: 100%|██████████| 118/118 [00:55<00:00,  2.12it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.984% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 116\n",
      "Epoch[116/116]: 100%|██████████| 116/116 [00:57<00:00,  2.00it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.985% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 115\n",
      "Epoch[115/115]: 100%|██████████| 115/115 [00:56<00:00,  2.03it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.986% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 113\n",
      "Epoch[113/113]: 100%|██████████| 113/113 [00:58<00:00,  1.92it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.986% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 112\n",
      "Epoch[112/112]: 100%|██████████| 112/112 [01:06<00:00,  1.67it/s]\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.695% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.696% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.987% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 111\n",
      "Epoch[38/111]:  33%|███▎      | 37/111 [00:21<00:34,  2.13it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for B in B_values:\n",
    "    # Split original training data into folds with sub-training and sub-calibration sets each\n",
    "    folds = NeuralProphet().crossvalidation_split_df(\n",
    "        train_df,\n",
    "        freq=freq,\n",
    "        k=B,\n",
    "        fold_pct=val_fold_pct,\n",
    "        fold_overlap_pct=(B*val_fold_pct - val_cov_pct)\n",
    "    )\n",
    "\n",
    "    # Train a bootstrap model on each sub-training set and predict on sub-calibration set per fold \n",
    "    for train_fold_df, cal_fold_df in folds:\n",
    "        if val_fold_size:\n",
    "            train_fold_len = int(len(cal_fold_df) * ((1-val_fold_size)/val_fold_size))\n",
    "            train_fold_df = train_fold_df[-train_fold_len: ]\n",
    "        # print(f\"  - Train start: {train_fold_df.ds.min()}, Train end: {train_fold_df.ds.max()}\")\n",
    "        # print(f\"  - Cal start:   {cal_fold_df.ds.min()}, Cal end:   {cal_fold_df.ds.max()}\")\n",
    "        # print(f\"  - Train shape:   {train_fold_df.shape}, Cal shape:   {cal_fold_df.shape}\")\n",
    "        m1 = NeuralProphet(**params)\n",
    "        set_random_seed(random_seed)\n",
    "        metrics = m1.fit(train_fold_df, freq=freq, minimal=True)\n",
    "        cal_forecast = m1.predict(cal_fold_df)\n",
    "        cal_forecast_dict[B] = pd.concat([cal_forecast_dict[B], cal_forecast[cols]], ignore_index=True)\n",
    "\n",
    "    # Aggregate the prediction for each sub-calibration row for all folds\n",
    "    cal_forecast_dict[B] = cal_forecast_dict[B].groupby('ds').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nonconformity_scores(scores, q, method):\n",
    "    plt.plot(scores, label=\"score\")\n",
    "    plt.axhline(y=q, color=\"r\", linestyle=\"-\", label=f\"q1={round(q, 2)}\")\n",
    "    plt.xlabel(\"Sorted Index\")\n",
    "    plt.ylabel(\"Nonconformity Score\")\n",
    "    plt.title(f\"{method} Nonconformity Score with q\")\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "qhat_df = pd.DataFrame(index=B_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enbpi_qhats = []\n",
    "for B in B_values:\n",
    "    enbpi_noncon_scores = np.abs(cal_forecast_dict[B]['residual1'].values)\n",
    "    enbpi_noncon_scores = enbpi_noncon_scores[~pd.isnull(enbpi_noncon_scores)]  # remove NaN values\n",
    "    enbpi_noncon_scores.sort()\n",
    "    # print(enbpi_noncon_scores.shape)\n",
    "\n",
    "    # get the q-hat index and value\n",
    "    enbpi_qhat_idx = int(len(enbpi_noncon_scores)*alpha)\n",
    "    enbpi_qhat = enbpi_noncon_scores[-enbpi_qhat_idx]\n",
    "    method = f'EnbPI_B{B}'\n",
    "    plot_nonconformity_scores(enbpi_noncon_scores, enbpi_qhat, method)\n",
    "    enbpi_qhats.append(enbpi_qhat)\n",
    "\n",
    "qhat_df['enbpi'] = enbpi_qhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enbcqr_scoring_func = (\n",
    "    lambda row: [None, None]\n",
    "    if row[f\"yhat1 {quantile_lo_str}%\"] is None or row[f\"yhat1 {quantile_hi_str}%\"] is None\n",
    "    else [\n",
    "        max(row[f\"yhat1 {quantile_lo_str}%\"] - row[\"y\"], row[\"y\"] - row[f\"yhat1 {quantile_hi_str}%\"]),\n",
    "        0 if row[f\"yhat1 {quantile_lo_str}%\"] - row[\"y\"] > row[\"y\"] - row[f\"yhat1 {quantile_hi_str}%\"] else 1,\n",
    "    ]\n",
    ")\n",
    "\n",
    "enbcqr_qhats = []\n",
    "for B in B_values:\n",
    "    enbcqr_scores_df = cal_forecast_dict[B].apply(enbcqr_scoring_func, axis=1, result_type=\"expand\")\n",
    "    enbcqr_scores_df.columns = [\"scores\", \"arg\"]\n",
    "    enbcqr_noncon_scores = enbcqr_scores_df[\"scores\"].values\n",
    "    enbcqr_noncon_scores = enbcqr_noncon_scores[~pd.isnull(enbcqr_noncon_scores)]  # remove NaN values\n",
    "    enbcqr_noncon_scores.sort()\n",
    "    print(enbcqr_noncon_scores.shape)\n",
    "\n",
    "    # get the q-hat index and value\n",
    "    enbcqr_qhat_idx = int(len(enbcqr_noncon_scores)*alpha)\n",
    "    enbcqr_qhat = enbcqr_noncon_scores[-enbcqr_qhat_idx]\n",
    "    method = f'EnbCQR_B{B}'\n",
    "    plot_nonconformity_scores(enbcqr_noncon_scores, enbcqr_qhat, method)\n",
    "    enbcqr_qhats.append(enbcqr_qhat)\n",
    "\n",
    "qhat_df['enbcqr'] = enbcqr_qhats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get forecasts and *EnbPI* and *EnbCQR* conformal prediction interval of OOS test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 7*24 # 0\n",
    "x_size = 12\n",
    "y_size = 6\n",
    "figsize = (x_size, y_size)\n",
    "plotting_backend = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enbpi_forecast_dict = defaultdict(pd.DataFrame)\n",
    "enbcqr_forecast_dict = defaultdict(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'enbpi'\n",
    "for B in B_values:\n",
    "    m1.q_hats = [qhat_df.loc[B, method]]\n",
    "    m1.conformal_method = 'naive'\n",
    "    m1.quantile_lo = None\n",
    "    m1.quantile_hi = None\n",
    "    enbpi_forecast = m1.predict(test_df)\n",
    "    # enbpi_forecast.head()\n",
    "    enbpi_forecast_dict[B] = enbpi_forecast\n",
    "    fig = m1.highlight_nth_step_ahead_of_each_forecast(1) \\\n",
    "            .plot(enbpi_forecast[-cutoff:], figsize=figsize, plotting_backend=plotting_backend)  # [-cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'enbcqr'\n",
    "for B in B_values:\n",
    "    m1.q_hats = [qhat_df.loc[B, method]]\n",
    "    m1.conformal_method = 'cqr'\n",
    "    m1.quantile_lo = quantile_lo_str\n",
    "    m1.quantile_hi = quantile_hi_str\n",
    "    enbcqr_forecast = m1.predict(test_df)\n",
    "    # enbcqr_forecast.head()\n",
    "    enbcqr_forecast_dict[B] = enbcqr_forecast\n",
    "    fig = m1.highlight_nth_step_ahead_of_each_forecast(1) \\\n",
    "            .plot(enbcqr_forecast[-cutoff:], figsize=figsize, plotting_backend=plotting_backend)  # [-cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance\n",
    "\n",
    "- **interval_width**: The average prediction interval, or q_hat multiplied by 2 because it is static or non-adaptive. Also the *efficiency* metric.\n",
    "- **miscoverage_rate**: The actual miscoverage error rate on the OOS test set. Also the *validity* metric.\n",
    "\n",
    "For the both metrics, the lower the number, the better the performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_values_str = [f'B_{B}' for B in B_values]\n",
    "conformal_methods = ['enbpi_tscv', 'enbcqr_tscv']\n",
    "eval_df = pd.DataFrame(columns=pd.MultiIndex.from_product([B_values_str, conformal_methods, ['interval_width','miscoverage_rate']]))\n",
    "eval_df = eval_df.reset_index().rename(columns={'index': 'model'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter in the model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['model'] = ['m3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter in the **interval_width** (*efficiency* metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate EnbCQR interval width function\n",
    "def calc_enbcqr_iw(fcast_df, qr_lo, qr_hi, qhat):\n",
    "    qr_lo_mean = fcast_df['yhat1'].mean() - fcast_df[f'yhat1 {qr_lo}%'].mean()\n",
    "    qr_hi_mean = fcast_df[f'yhat1 {qr_hi}%'].mean() - fcast_df['yhat1'].mean()\n",
    "    return qr_lo_mean + qr_hi_mean + qhat*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for B in B_values:\n",
    "    # enbpi interval width\n",
    "    enbpi_iw = [qhat_df.loc[B, 'enbpi']*2]\n",
    "    eval_df[f'B_{B}', 'enbpi_tscv', 'interval_width'] = enbpi_iw\n",
    "\n",
    "    # EnbCQR interval width\n",
    "    enbcqr_iw = calc_enbcqr_iw(enbcqr_forecast, quantile_lo_str, quantile_hi_str, qhat_df.loc[B, 'enbcqr'])\n",
    "    # cqr_iw1 = calc_cqr_iw(cqr_forecast1, quantile_lo_str, quantile_hi_str, cqr_qhat1)\n",
    "    # cqr_iw2 = calc_cqr_iw(cqr_forecast2, quantile_lo_str, quantile_hi_str, cqr_qhat2)\n",
    "    # cqr_iw3 = calc_cqr_iw(cqr_forecast3, quantile_lo_str, quantile_hi_str, cqr_qhat3)\n",
    "    # cqr_iw4 = calc_cqr_iw(cqr_forecast4, quantile_lo_str, quantile_hi_str, cqr_qhat4)\n",
    "    enbcqr_iw = [enbcqr_iw]\n",
    "    # cqr_iw = [cqr_iw1, cqr_iw2, cqr_iw3, cqr_iw4]\n",
    "    eval_df[f'B_{B}', 'enbcqr_tscv', 'interval_width'] = enbcqr_iw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter in the **miscoverage rate** (*validity* metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate EnbPI miscoverage rate function\n",
    "def calc_enbpi_mr(fcast_df):\n",
    "    n_covered = fcast_df.apply(lambda row: bool(row['yhat1 - qhat1'] <= row['y'] <= row['yhat1 + qhat1']), axis=1)\n",
    "    coverage_rate = n_covered.sum() / len(fcast_df)\n",
    "    return 1 - coverage_rate\n",
    "\n",
    "# Calculate EnbCQR and EnbCQR Advanced miscoverage rate function\n",
    "def calc_enbcqr_mr(fcast_df, qr_lo, qr_hi):\n",
    "    n_covered = fcast_df.apply(lambda row: bool(row[f'yhat1 {qr_lo}% - qhat1'] <= row['y'] <= row[f'yhat1 {qr_hi}% + qhat1']), axis=1)\n",
    "    coverage_rate = n_covered.sum() / len(fcast_df)\n",
    "    return 1 - coverage_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for B in B_values:\n",
    "    # enbpi miscoverage rate\n",
    "    enbpi_mr = calc_enbpi_mr(enbpi_forecast_dict[B])\n",
    "    # enbpi_mr1 = calc_enbpi_mr(enbpi_forecast1)\n",
    "    # enbpi_mr2 = calc_enbpi_mr(enbpi_forecast2)\n",
    "    # enbpi_mr3 = calc_enbpi_mr(enbpi_forecast3)\n",
    "    # enbpi_mr4 = calc_enbpi_mr(enbpi_forecast4)\n",
    "    enbpi_mr = [enbpi_mr]\n",
    "    # enbpi_mr = [enbpi_mr1, enbpi_mr2, enbpi_mr3, enbpi_mr4]]\n",
    "    eval_df[f'B_{B}', 'enbpi_tscv', 'miscoverage_rate'] = enbpi_mr\n",
    "\n",
    "    # CQR miscoverage rate\n",
    "    enbcqr_mr = calc_enbcqr_mr(enbcqr_forecast_dict[B], quantile_lo_str, quantile_hi_str)\n",
    "    # cqr_mr1 = calc_cqr_mr(cqr_forecast1, quantile_lo_str, quantile_hi_str)\n",
    "    # cqr_mr2 = calc_cqr_mr(cqr_forecast2, quantile_lo_str, quantile_hi_str)\n",
    "    # cqr_mr3 = calc_cqr_mr(cqr_forecast3, quantile_lo_str, quantile_hi_str)\n",
    "    # cqr_mr4 = calc_cqr_mr(cqr_forecast4, quantile_lo_str, quantile_hi_str)\n",
    "    enbcqr_mr = [enbcqr_mr]\n",
    "    # cqr_mr = [cqr_mr1, cqr_mr2, cqr_mr3, cqr_mr4]\n",
    "    eval_df[f'B_{B}', 'enbcqr_tscv', 'miscoverage_rate'] = enbcqr_mr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show evaluation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: \n",
    "\n",
    "CQR outputs narrower prediction *interval width* than Naive while maintaining slightly better *miscoverage rate* (for the advanced AR models m4 and m4). The complex m4 model performs the best for *interval_width* while the simple m1 performs the best for actual *miscoverage rate*, hovering around *alpha*. However, that is because it has also by far the highest *interval_width*. m3 and m4 models have actual *miscoverage rates* are roughly twice the amount as the intended *alpha* of 0.1, which is consistent with the conformal prediction literature."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "energy_data_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dev:Python",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9896633dd2687027a97d37c5dc67af73c090796eacc50847f77e025856fff9f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "24bf564f55644476911a6cf004a395e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aab682cd3df24821a80331720f7c24e5",
      "placeholder": "​",
      "style": "IPY_MODEL_f35fc9cbd82c4187a4cdc08c3ac26998",
      "value": " 264/297 [00:04&lt;00:00, 59.93it/s]"
     }
    },
    "2d8235496ec642af8192f52d9f2692b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a192ccc35e94e9f8be85898ed583e2c",
      "placeholder": "​",
      "style": "IPY_MODEL_87c170d1e00742a29e7f797e98c49cc2",
      "value": " 89%"
     }
    },
    "4ac0917121f8498698e087259b787dcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d8235496ec642af8192f52d9f2692b1",
       "IPY_MODEL_c94a8ae41b994c55a96ad44806b0f1c7",
       "IPY_MODEL_24bf564f55644476911a6cf004a395e7"
      ],
      "layout": "IPY_MODEL_94108fe9090f47c7ba2216479e0d3fac"
     }
    },
    "87c170d1e00742a29e7f797e98c49cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a192ccc35e94e9f8be85898ed583e2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94108fe9090f47c7ba2216479e0d3fac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9467345334da47a8beadc770feef952a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aab682cd3df24821a80331720f7c24e5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c94a8ae41b994c55a96ad44806b0f1c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc468cd35d2b4f0e8eb287689ac15412",
      "max": 297,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9467345334da47a8beadc770feef952a",
      "value": 264
     }
    },
    "dc468cd35d2b4f0e8eb287689ac15412": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f35fc9cbd82c4187a4cdc08c3ac26998": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
